
namespace: monitoring

storageClass: 
  name: fast
  provisioner: kubernetes.io/gce-pd

alertmanager:
  image: prom/alertmanager:v0.20.0
  service:
    type: ClusterIP
  config: |-
    global:
      resolve_timeout: 5m
      slack_api_url: "<your_slack_hook>"
      victorops_api_url: "<your_victorops_hook>"

    templates:
    - '/etc/alertmanager-templates/*.tmpl'
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 1m
      repeat_interval: 5m  
      receiver: default 
      routes:
      - match:
          team: devops
        receiver: devops
        continue: true 
      - match: 
          team: dev
        receiver: dev
        continue: true

    receivers:
    - name: 'default'

    - name: 'devops'
      victorops_configs:
      - api_key: '<YOUR_API_KEY>'
        routing_key: 'devops'
        message_type: 'CRITICAL'
        entity_display_name: '{{ .CommonLabels.alertname }}'
        state_message: 'Alert: {{ .CommonLabels.alertname }}. Summary:{{ .CommonAnnotations.summary }}. RawData: {{ .CommonLabels }}'
      slack_configs:
      - channel: '#k8-alerts'
        send_resolved: true


    - name: 'dev'
      victorops_configs:
      - api_key: '<YOUR_API_KEY>'
        routing_key: 'dev'
        message_type: 'CRITICAL'
        entity_display_name: '{{ .CommonLabels.alertname }}'
        state_message: 'Alert: {{ .CommonLabels.alertname }}. Summary:{{ .CommonAnnotations.summary }}. RawData: {{ .CommonLabels }}'
      slack_configs:
      - channel: '#k8-alerts'
        send_resolved: true  

grafana:
  image: grafana/grafana:6.6.1
  service:
    type: ClusterIP
  storage: 10Gi

kube_state_metrics:
  image: quay.io/mxinden/kube-state-metrics:v1.4.0-gzip.3

prometheus:
  image: prom/prometheus:v2.14.0
  replicas: 3
  storage: 20Gi
  resources: 
    limits:
      cpu: "1"
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 256Mi
  config: |-
    global:
      scrape_interval: 5s
      evaluation_interval: 5s
      external_labels:
        cluster: prometheus-ha
        # Each Prometheus has to have unique labels.
        prometheus_replica: $(POD_NAME)

    rule_files:
      - /etc/prometheus/rules/*rules.yaml

    alerting:

      # We want our alerts to be deduplicated
      # from different replicas.
      alert_relabel_configs:
      - regex: prometheus_replica
        action: labeldrop

      alertmanagers:
        - scheme: http
          path_prefix: /
          static_configs:
            - targets: ['alertmanager:9093']

    scrape_configs:
    - job_name: kubernetes-nodes-cadvisor
      scrape_interval: 10s
      scrape_timeout: 10s
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        # Only for Kubernetes ^1.7.3.
        # See: https://github.com/prometheus/prometheus/issues/2916
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
      metric_relabel_configs:
        - action: replace
          source_labels: [id]
          regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
          target_label: rkt_container_name
          replacement: '${2}-${1}'
        - action: replace
          source_labels: [id]
          regex: '^/system\.slice/(.+)\.service$'
          target_label: systemd_service_name
          replacement: '${1}'

    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2


    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
        - role: endpoints
      scheme: https 
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

    - job_name: 'kubernetes-service-endpoints'
      kubernetes_sd_configs:
        - role: endpoints
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: (.+)(?::\d+);(\d+)
          replacement: $1:$2

  rules:
    config: |-
      groups:
        - name: Deployment
          rules:
          - alert: Deployment at 0 Replicas
            annotations:
              summary: Deployment {{$labels.deployment}} is currently having no pods running
              description: "\nCluster Name: {{$externalLabels.cluster}}\nNamespace: {{$labels.namespace}}\nDeployment name: {{$labels.deployment}}\n" 
            expr: |
              sum(kube_deployment_status_replicas{pod_template_hash=""}) by (deployment,namespace)  < 1
            for: 1m
            labels:
              team: devops

          - alert: HPA Scaling Limited  
            annotations: 
              summary: HPA named {{$labels.hpa}} in {{$labels.namespace}} namespace has reached scaling limited state
              description: "\nCluster Name: {{$externalLabels.cluster}}\nNamespace: {{$labels.namespace}}\nHPA name: {{$labels.hpa}}\n" 
            expr: | 
              (sum(kube_hpa_status_condition{condition="ScalingLimited",status="true"}) by (hpa,namespace)) == 1
            for: 1m
            labels: 
              team: devops

          - alert: HPA at MaxCapacity 
            annotations: 
              summary: HPA named {{$labels.hpa}} in {{$labels.namespace}} namespace is running at Max Capacity
              description: "\nCluster Name: {{$externalLabels.cluster}}\nNamespace: {{$labels.namespace}}\nHPA name: {{$labels.hpa}}\n" 
            expr: | 
              ((sum(kube_hpa_spec_max_replicas) by (hpa,namespace)) - (sum(kube_hpa_status_current_replicas) by (hpa,namespace))) == 0
            for: 1m
            labels: 
              team: devops

        - name: Pods
          rules:
          - alert: Container restarted
            annotations:
              summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} was restarted
              description: "\nCluster Name: {{$externalLabels.cluster}}\nNamespace: {{$labels.namespace}}\nPod name: {{$labels.pod}}\nContainer name: {{$labels.container}}\n" 
            expr: |
              sum(increase(kube_pod_container_status_restarts_total{namespace!="kube-system",pod_template_hash=""}[1m])) by (pod,namespace,container) > 0
            for: 0m
            labels:
              team: dev

          - alert: High Memory Usage of Container 
            annotations: 
              summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is using more than 75% of Memory Limit
              description: "\nCluster Name: {{$externalLabels.cluster}}\nNamespace: {{$labels.namespace}}\nPod name: {{$labels.pod}}\nContainer name: {{$labels.container}}\n" 
            expr: | 
              ((( sum(container_memory_working_set_bytes{image!="",container_name!="POD", namespace!="kube-system"}) by (namespace,container_name,pod_name)  / sum(container_spec_memory_limit_bytes{image!="",container_name!="POD",namespace!="kube-system"}) by (namespace,container_name,pod_name) ) * 100 ) < +Inf ) > 75
            for: 5m
            labels: 
              team: dev

          - alert: High CPU Usage of Container 
            annotations: 
              summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is using more than 75% of CPU Limit
              description: "\nCluster Name: {{$externalLabels.cluster}}\nNamespace: {{$labels.namespace}}\nPod name: {{$labels.pod}}\nContainer name: {{$labels.container}}\n" 
            expr: | 
              ((sum(irate(container_cpu_usage_seconds_total{image!="",container_name!="POD", namespace!="kube-system"}[30s])) by (namespace,container_name,pod_name) / sum(container_spec_cpu_quota{image!="",container_name!="POD", namespace!="kube-system"} / container_spec_cpu_period{image!="",container_name!="POD", namespace!="kube-system"}) by (namespace,container_name,pod_name) ) * 100)  > 75
            for: 5m
            labels: 
              team: dev

        - name: Nodes
          rules:
          - alert: High Node Memory Usage
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 80% memory used. Plan Capcity
              description: "\nCluster Name: {{$externalLabels.cluster}}\Node: {{$labels.kubernetes_io_hostname}}\n" 
            expr: |
              (sum (container_memory_working_set_bytes{id="/",container_name!="POD"}) by (kubernetes_io_hostname) / sum (machine_memory_bytes{}) by (kubernetes_io_hostname) * 100) > 80
            for: 5m
            labels:
              team: devops

          - alert: High Node CPU Usage
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 80% allocatable cpu used. Plan Capacity.
              description: "\nCluster Name: {{$externalLabels.cluster}}\Node: {{$labels.kubernetes_io_hostname}}\n" 
            expr: |
              (sum(rate(container_cpu_usage_seconds_total{id="/", container_name!="POD"}[1m])) by (kubernetes_io_hostname) / sum(machine_cpu_cores) by (kubernetes_io_hostname)  * 100) > 80
            for: 5m
            labels:
              team: devops

          - alert: High Node Disk Usage
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 85% disk used. Plan Capacity.
              description: "\nCluster Name: {{$externalLabels.cluster}}\Node: {{$labels.kubernetes_io_hostname}}\n" 
            expr: |
              (sum(container_fs_usage_bytes{device=~"^/dev/[sv]d[a-z][1-9]$",id="/",container_name!="POD"}) by (kubernetes_io_hostname) / sum(container_fs_limit_bytes{container_name!="POD",device=~"^/dev/[sv]d[a-z][1-9]$",id="/"}) by (kubernetes_io_hostname)) * 100 > 85
            for: 5m
            labels:
              team: devops

pushgateway:
  image: prom/pushgateway:v1.0.1
  replicas: 1
  service:
    type: ClusterIP

thanos:
  archive_bucket: prometheus-long-term
  gcs_secret: thanos-gcs-credentials
  image: quay.io/thanos/thanos:v0.12.0 

  query:
    replicas: 1
    resources: 
      limits:
        cpu: "1"
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 256Mi
    service:
      type: ClusterIP  

  compactor:
    replicas: 1
    resources: 
      limits:
        cpu: "1"
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 256Mi

  store:
    replicas: 1
    resources:
      limits:
        cpu: 1
        memory: 8Gi
      requests:
        cpu: 0.5
        memory: 1Gi

  ruler:
    replicas: 1
    rules_bucket: thanos-ruler   
    service:
      type: ClusterIP
    resources: 
      limits:
        cpu: "1"
        memory: 1Gi
      requests:
        cpu: 50m
        memory: 100Mi
    config: |-
      groups:
      - name: metamonitoring
        rules:
        - alert: PrometheusReplicaDown
          annotations:
            message: Prometheus replica in cluster {{$labels.cluster}} has disappeared from Prometheus target discovery.
          expr: |
            sum(up{cluster="prometheus-ha", instance=~".*:9090", job="kubernetes-service-endpoints"}) by (job,cluster) < 3
          for: 15s
          labels:
            severity: critical

ingress:
  enabled: true
  labels: {}
    # traffic: external
  annotations: 
    kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  grafana:
    host: grafana.yourdomain.com
  alertmanager:
    host: alertmanager.yourdomain.com
  query:
    host: thanos-query.yourdomain.com
  ruler:
    host: thanos-ruler.yourdomain.com 


